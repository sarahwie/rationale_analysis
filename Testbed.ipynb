{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [json.loads(line) for line in open('Datasets/SST/data/dev.jsonl')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [d['label'] for d in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths_r = np.array([len(d['rationales']) for d in tao_rationale])\n",
    "lengths_d = np.array([len(d['metadata']['tokens']) for d in rationales])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = {'top_k' : 'top k', 'max_length' : 'top k contiguous'}\n",
    "for saliency in ['wrapper'] : #, 'simple_gradient', 'integrated_gradient'] :\n",
    "    for rat in ['top_k', 'max_length'] :\n",
    "        ps = []\n",
    "        acc = []\n",
    "        for p in [20, 40, 60, 80, 100] :\n",
    "            rationales = [json.loads(line) \n",
    "                          for line in open(f'outputs/bert_classification/SST/bert_base_uncased/{saliency}_saliency/{rat}_rationale/MAX_LENGTH_PERCENT={p}/dev.jsonl')]\n",
    "            lengths_d = np.array([len(x['document'].split()) for x in data[:len(rationales)]])\n",
    "            lengths_r = np.array([len(x['document'].split()) for x in rationales])\n",
    "            metrics = json.load(open(f'outputs/bert_classification/SST/bert_base_uncased/{saliency}_saliency/{rat}_rationale/MAX_LENGTH_PERCENT={p}/model_b/metrics.json'))\n",
    "            print(np.mean(lengths_r/lengths_d), metrics['best_validation_accuracy'])\n",
    "            ps.append(np.mean(lengths_r/lengths_d))\n",
    "            acc.append(metrics['best_validation_accuracy'])\n",
    "        plt.plot([0] + ps, [0.509] + acc, label= '[CLS] Attention +' + name[rat])\n",
    "        \n",
    "ps , acc = [], []\n",
    "for r in [0.2, 0.4, 0.6, 0.8, 1.0] :\n",
    "    metrics = json.load(open(f'outputs/bert_encoder_generator/SST/run_1_enc_gen/{r}/metrics.json'))\n",
    "    ps.append(metrics['best_validation__rat_length'])\n",
    "    acc.append(metrics['best_validation_accuracy'])\n",
    "    \n",
    "plt.scatter(ps, acc, label='Tao')\n",
    "        \n",
    "plt.xlabel('Percentage of Document Selected')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.tight_layout()\n",
    "plt.legend()\n",
    "plt.savefig('sst-max-length-length.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.data.token_indexers import PretrainedBertIndexer\n",
    "from allennlp.data.tokenizers import Token\n",
    "from allennlp.data.vocabulary import Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = PretrainedBertIndexer(do_lowercase=True, max_pieces=100, pretrained_model='bert-base-uncased', truncate_long_sequences=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [Token(str(i)) for i in range(101)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index.tokens_to_indices(tokens, vocab, index_name='bert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_transformers import BertTokenizer, BertForTokenClassification\n",
    "import torch\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForTokenClassification.from_pretrained('bert-base-uncased')\n",
    "input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\")).unsqueeze(0)  # Batch size 1\n",
    "labels = torch.tensor([1] * input_ids.size(1)).unsqueeze(0)  # Batch size 1\n",
    "outputs = model(input_ids, labels=labels)\n",
    "loss, scores = outputs[:2]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
